{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM/iUT3L4amNZi2A9nSyoqm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/1273474/flipkart-ba-churn-project/blob/main/Customer_Churn_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2uldLRQW4Ryr",
        "outputId": "c0f3ee9a-1aa7-4f40-ae6d-3bc82fa27d4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.12/dist-packages (1.7.4.5)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.12/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2025.10.5)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.12/dist-packages (from kaggle) (3.4.4)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from kaggle) (3.11)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from kaggle) (5.29.5)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.12/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.32.4)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from kaggle) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.12/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.12/dist-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.5.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.12/dist-packages (from kaggle) (0.5.1)\n",
            "Collecting causalml\n",
            "  Downloading causalml-0.15.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Collecting forestci==0.6 (from causalml)\n",
            "  Downloading forestci-0.6-py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting pathos==0.2.9 (from causalml)\n",
            "  Downloading pathos-0.2.9-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from causalml) (2.0.2)\n",
            "Collecting scipy<1.16.0,>=1.4.1 (from causalml)\n",
            "  Downloading scipy-1.15.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from causalml) (3.10.0)\n",
            "Requirement already satisfied: pandas>=0.24.1 in /usr/local/lib/python3.12/dist-packages (from causalml) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from causalml) (1.6.1)\n",
            "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from causalml) (0.14.5)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (from causalml) (0.13.2)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.12/dist-packages (from causalml) (3.1.0)\n",
            "Requirement already satisfied: pydotplus in /usr/local/lib/python3.12/dist-packages (from causalml) (2.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from causalml) (4.67.1)\n",
            "Requirement already satisfied: shap in /usr/local/lib/python3.12/dist-packages (from causalml) (0.49.1)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.12/dist-packages (from causalml) (0.3.8)\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.12/dist-packages (from causalml) (4.6.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from causalml) (25.0)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.12/dist-packages (from causalml) (0.21)\n",
            "Collecting ppft>=1.7.6.5 (from pathos==0.2.9->causalml)\n",
            "  Downloading ppft-1.7.7-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting pox>=0.3.1 (from pathos==0.2.9->causalml)\n",
            "  Downloading pox-0.3.6-py3-none-any.whl.metadata (8.0 kB)\n",
            "Requirement already satisfied: multiprocess>=0.70.13 in /usr/local/lib/python3.12/dist-packages (from pathos==0.2.9->causalml) (0.70.16)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24.1->causalml) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24.1->causalml) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24.1->causalml) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.6.0->causalml) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.6.0->causalml) (3.6.0)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.12/dist-packages (from statsmodels>=0.9.0->causalml) (1.0.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->causalml) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->causalml) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->causalml) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->causalml) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->causalml) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->causalml) (3.2.5)\n",
            "Requirement already satisfied: slicer==0.0.8 in /usr/local/lib/python3.12/dist-packages (from shap->causalml) (0.0.8)\n",
            "Requirement already satisfied: numba>=0.54 in /usr/local/lib/python3.12/dist-packages (from shap->causalml) (0.60.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.12/dist-packages (from shap->causalml) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from shap->causalml) (4.15.0)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.12/dist-packages (from xgboost->causalml) (2.27.3)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.54->shap->causalml) (0.43.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=0.24.1->causalml) (1.17.0)\n",
            "Downloading causalml-0.15.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m63.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading forestci-0.6-py3-none-any.whl (12 kB)\n",
            "Downloading pathos-0.2.9-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.15.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.3/37.3 MB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pox-0.3.6-py3-none-any.whl (29 kB)\n",
            "Downloading ppft-1.7.7-py3-none-any.whl (56 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: scipy, ppft, pox, pathos, forestci, causalml\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.16.2\n",
            "    Uninstalling scipy-1.16.2:\n",
            "      Successfully uninstalled scipy-1.16.2\n",
            "Successfully installed causalml-0.15.5 forestci-0.6 pathos-0.2.9 pox-0.3.6 ppft-1.7.7 scipy-1.15.3\n",
            "Collecting econml\n",
            "  Downloading econml-0.16.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (37 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from econml) (2.0.2)\n",
            "Requirement already satisfied: scipy>1.4.0 in /usr/local/lib/python3.12/dist-packages (from econml) (1.15.3)\n",
            "Requirement already satisfied: scikit-learn<1.7,>=1.0 in /usr/local/lib/python3.12/dist-packages (from econml) (1.6.1)\n",
            "Collecting sparse (from econml)\n",
            "  Downloading sparse-0.17.0-py2.py3-none-any.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: joblib>=0.13.0 in /usr/local/lib/python3.12/dist-packages (from econml) (1.5.2)\n",
            "Requirement already satisfied: statsmodels>=0.10 in /usr/local/lib/python3.12/dist-packages (from econml) (0.14.5)\n",
            "Requirement already satisfied: pandas>1.0 in /usr/local/lib/python3.12/dist-packages (from econml) (2.2.2)\n",
            "Collecting shap<0.49.0,>=0.38.1 (from econml)\n",
            "  Downloading shap-0.48.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.12/dist-packages (from econml) (4.6.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from econml) (25.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>1.0->econml) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>1.0->econml) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>1.0->econml) (2025.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn<1.7,>=1.0->econml) (3.6.0)\n",
            "Requirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.12/dist-packages (from shap<0.49.0,>=0.38.1->econml) (4.67.1)\n",
            "Requirement already satisfied: slicer==0.0.8 in /usr/local/lib/python3.12/dist-packages (from shap<0.49.0,>=0.38.1->econml) (0.0.8)\n",
            "Requirement already satisfied: numba>=0.54 in /usr/local/lib/python3.12/dist-packages (from shap<0.49.0,>=0.38.1->econml) (0.60.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.12/dist-packages (from shap<0.49.0,>=0.38.1->econml) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from shap<0.49.0,>=0.38.1->econml) (4.15.0)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.12/dist-packages (from statsmodels>=0.10->econml) (1.0.2)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.54->shap<0.49.0,>=0.38.1->econml) (0.43.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>1.0->econml) (1.17.0)\n",
            "Downloading econml-0.16.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m54.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading shap-0.48.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sparse-0.17.0-py2.py3-none-any.whl (259 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m259.4/259.4 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sparse, shap, econml\n",
            "  Attempting uninstall: shap\n",
            "    Found existing installation: shap 0.49.1\n",
            "    Uninstalling shap-0.49.1:\n",
            "      Successfully uninstalled shap-0.49.1\n",
            "Successfully installed econml-0.16.0 shap-0.48.0 sparse-0.17.0\n",
            "Requirement already satisfied: shap in /usr/local/lib/python3.12/dist-packages (0.48.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from shap) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from shap) (1.15.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from shap) (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from shap) (2.2.2)\n",
            "Requirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.12/dist-packages (from shap) (4.67.1)\n",
            "Requirement already satisfied: packaging>20.9 in /usr/local/lib/python3.12/dist-packages (from shap) (25.0)\n",
            "Requirement already satisfied: slicer==0.0.8 in /usr/local/lib/python3.12/dist-packages (from shap) (0.0.8)\n",
            "Requirement already satisfied: numba>=0.54 in /usr/local/lib/python3.12/dist-packages (from shap) (0.60.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.12/dist-packages (from shap) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from shap) (4.15.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.54->shap) (0.43.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->shap) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->shap) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->shap) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->shap) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->shap) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->shap) (1.17.0)\n",
            "Libraries installed and imported successfully.\n"
          ]
        }
      ],
      "source": [
        "# --- 1.A: Install Necessary Libraries ---\n",
        "# We install the libraries that are not pre-installed on Colab.\n",
        "!pip install kaggle\n",
        "!pip install causalml\n",
        "!pip install econml\n",
        "!pip install shap\n",
        "\n",
        "# --- Import Libraries ---\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import glob # This helps in finding file paths\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "# ML Libraries\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Set a random seed for reproducibility.\n",
        "# This ensures that every time we run the code, we get the same \"random\" results.\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "\n",
        "print(\"Libraries installed and imported successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1.B.2: Upload Kaggle API Key ---\n",
        "# This code will open an upload dialog.\n",
        "# Please upload the 'kaggle.json' file you just downloaded.\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "print(\"Please upload your kaggle.json file:\")\n",
        "files.upload()\n",
        "\n",
        "print(\"File uploaded.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "id": "OpF29bYt4cR5",
        "outputId": "8d5bf465-e530-4cb9-944b-7a9a266a5371"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please upload your kaggle.json file:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-66e2a874-26f6-4cfa-b3e7-ee41e1866704\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-66e2a874-26f6-4cfa-b3e7-ee41e1866704\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle (1).json to kaggle (1) (1).json\n",
            "File uploaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1.B.3: Download and Unzip the Olist Dataset (Corrected) ---\n",
        "\n",
        "# Set up the Kaggle directory\n",
        "!mkdir -p ~/.kaggle\n",
        "\n",
        "# --- THIS IS THE FIX ---\n",
        "# Move the renamed file \"kaggle (1).json\" to the correct location\n",
        "!mv \"kaggle (1).json\" ~/.kaggle/kaggle.json\n",
        "# --- END OF FIX ---\n",
        "\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "print(\"Downloading Olist dataset from Kaggle...\")\n",
        "# This is the API command for the dataset\n",
        "!kaggle datasets download -d olistbr/brazilian-ecommerce\n",
        "\n",
        "print(\"Download complete. Unzipping files...\")\n",
        "# -o flag will overwrite existing files without asking\n",
        "!unzip -o brazilian-ecommerce.zip -d data\n",
        "\n",
        "print(\"All files unzipped into the 'data' folder.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TEbqYMO15Zx2",
        "outputId": "09235d44-6ef4-494d-9db1-17ee83ca1443"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading Olist dataset from Kaggle...\n",
            "Dataset URL: https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce\n",
            "License(s): CC-BY-NC-SA-4.0\n",
            "Downloading brazilian-ecommerce.zip to /content\n",
            "  0% 0.00/42.6M [00:00<?, ?B/s]\n",
            "100% 42.6M/42.6M [00:00<00:00, 1.31GB/s]\n",
            "Download complete. Unzipping files...\n",
            "Archive:  brazilian-ecommerce.zip\n",
            "  inflating: data/olist_customers_dataset.csv  \n",
            "  inflating: data/olist_geolocation_dataset.csv  \n",
            "  inflating: data/olist_order_items_dataset.csv  \n",
            "  inflating: data/olist_order_payments_dataset.csv  \n",
            "  inflating: data/olist_order_reviews_dataset.csv  \n",
            "  inflating: data/olist_orders_dataset.csv  \n",
            "  inflating: data/olist_products_dataset.csv  \n",
            "  inflating: data/olist_sellers_dataset.csv  \n",
            "  inflating: data/product_category_name_translation.csv  \n",
            "All files unzipped into the 'data' folder.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1.B.4: Load all CSV files into a dictionary of DataFrames ---\n",
        "\n",
        "# Define the path to our data folder\n",
        "data_path = 'data/'\n",
        "\n",
        "# Get a list of all csv files in the data folder\n",
        "all_files = glob.glob(os.path.join(data_path, \"*.csv\"))\n",
        "\n",
        "# Create a dictionary to hold our dataframes\n",
        "# The key will be the file name (e.g., 'olist_orders_dataset')\n",
        "# The value will be the actual pandas DataFrame\n",
        "data = {}\n",
        "\n",
        "print(\"Loading files...\")\n",
        "for f in all_files:\n",
        "    # Get the file name from the path\n",
        "    file_name = os.path.basename(f).replace('.csv', '')\n",
        "\n",
        "    # Read the csv file into a DataFrame and store it in our dictionary\n",
        "    data[file_name] = pd.read_csv(f)\n",
        "    print(f\"Loaded: {file_name}\")\n",
        "\n",
        "print(\"\\n--- All Data Loaded Successfully ---\")\n",
        "# Let's check the keys (our table names)\n",
        "print(\"Available tables:\", data.keys())\n",
        "\n",
        "# Let's inspect the 'orders' table as an example\n",
        "print(\"\\n--- Example: 'olist_orders_dataset' head ---\")\n",
        "print(data['olist_orders_dataset'].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iiAfubb85hKD",
        "outputId": "990c8cae-e9d9-4f96-f779-236053cf0916"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading files...\n",
            "Loaded: olist_order_payments_dataset\n",
            "Loaded: olist_order_items_dataset\n",
            "Loaded: olist_geolocation_dataset\n",
            "Loaded: olist_sellers_dataset\n",
            "Loaded: olist_order_reviews_dataset\n",
            "Loaded: olist_orders_dataset\n",
            "Loaded: product_category_name_translation\n",
            "Loaded: olist_customers_dataset\n",
            "Loaded: olist_products_dataset\n",
            "\n",
            "--- All Data Loaded Successfully ---\n",
            "Available tables: dict_keys(['olist_order_payments_dataset', 'olist_order_items_dataset', 'olist_geolocation_dataset', 'olist_sellers_dataset', 'olist_order_reviews_dataset', 'olist_orders_dataset', 'product_category_name_translation', 'olist_customers_dataset', 'olist_products_dataset'])\n",
            "\n",
            "--- Example: 'olist_orders_dataset' head ---\n",
            "                           order_id                       customer_id  \\\n",
            "0  e481f51cbdc54678b7cc49136f2d6af7  9ef432eb6251297304e76186b10a928d   \n",
            "1  53cdb2fc8bc7dce0b6741e2150273451  b0830fb4747a6c6d20dea0b8c802d7ef   \n",
            "2  47770eb9100c2d0c44946d9cf07ec65d  41ce2a54c0b03bf3443c3d931a367089   \n",
            "3  949d5b44dbf5de918fe9c16f97b45f8a  f88197465ea7920adcdbec7375364d82   \n",
            "4  ad21c59c0840e6cb83a9ceb5573f8159  8ab97904e6daea8866dbdbc4fb7aad2c   \n",
            "\n",
            "  order_status order_purchase_timestamp    order_approved_at  \\\n",
            "0    delivered      2017-10-02 10:56:33  2017-10-02 11:07:15   \n",
            "1    delivered      2018-07-24 20:41:37  2018-07-26 03:24:27   \n",
            "2    delivered      2018-08-08 08:38:49  2018-08-08 08:55:23   \n",
            "3    delivered      2017-11-18 19:28:06  2017-11-18 19:45:59   \n",
            "4    delivered      2018-02-13 21:18:39  2018-02-13 22:20:29   \n",
            "\n",
            "  order_delivered_carrier_date order_delivered_customer_date  \\\n",
            "0          2017-10-04 19:55:00           2017-10-10 21:25:13   \n",
            "1          2018-07-26 14:31:00           2018-08-07 15:27:45   \n",
            "2          2018-08-08 13:50:00           2018-08-17 18:06:29   \n",
            "3          2017-11-22 13:39:59           2017-12-02 00:28:42   \n",
            "4          2018-02-14 19:46:34           2018-02-16 18:17:02   \n",
            "\n",
            "  order_estimated_delivery_date  \n",
            "0           2017-10-18 00:00:00  \n",
            "1           2018-08-13 00:00:00  \n",
            "2           2018-09-04 00:00:00  \n",
            "3           2017-12-15 00:00:00  \n",
            "4           2018-02-26 00:00:00  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ztmA8PAp5m11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1.C: Data Dictionary\n",
        "\n",
        "This document outlines the schema and business meaning of the key tables used in this project.\n",
        "\n",
        "| File Name | Purpose | Key Columns (and what they mean) |\n",
        "| :--- | :--- | :--- |\n",
        "| **`olist_customers_dataset`** | Holds customer information. | `customer_id`: Key for this table (links to `orders`). <br> `customer_unique_id`: The *true* ID for a customer (one customer can have multiple `customer_id`s for different orders). <br> `customer_zip_code_prefix`: Location of the customer. |\n",
        "| **`olist_orders_dataset`** | The main table, lists all orders. | `order_id`: The unique ID for an order. **This is our main key.** <br> `customer_id`: Links to the `customers` table. <br> `order_status`: (e.g., 'delivered', 'shipped', 'canceled'). <br> `order_purchase_timestamp`: When the order was placed. |\n",
        "| **`olist_order_items_dataset`** | Connects orders to products. | `order_id`: Links to the `orders` table. <br> `product_id`: Links to the `products` table. <br> `seller_id`: Links to the `sellers` table. <br> `price`: The price of the item. <br> `freight_value`: The shipping cost. |\n",
        "| **`olist_order_payments_dataset`** | Contains all payment information. | `order_id`: Links to the `orders` table. <br> `payment_type`: (e.g., 'credit_card', 'boleto'). <br> `payment_value`: Total value of the payment. |\n",
        "| **`olist_order_reviews_dataset`** | Contains customer reviews. | `order_id`: Links to the `orders` table. <br> `review_score`: The rating (1 to 5). **Key driver for churn.** <br> `review_comment_message`: (Text data, advanced step). |\n",
        "| **`olist_products_dataset`** | Contains product information. | `product_id`: Links to `order_items`. <br> `product_category_name`: (e.g., 'beleza_saude', 'informatica_acessorios'). |\n",
        "| **`olist_sellers_dataset`** | Contains seller information. | `seller_id`: Links to `order_items`. <br> `seller_zip_code_prefix`: Location of the seller. |\n",
        "| **`product_category_name_translation`** | Translates category names. | `product_category_name`: The Portuguese name. <br> `product_category_name_english`: The English name. |"
      ],
      "metadata": {
        "id": "NYtvrQFY6_gp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 2.A: High-Level Statistics ---\n",
        "print(\"--- Starting Step 2: Exploratory Data Analysis ---\")\n",
        "\n",
        "# First, let's see how many orders we actually have.\n",
        "print(f\"Total rows in orders table: {len(data['olist_orders_dataset'])}\")\n",
        "\n",
        "# But how many unique orders?\n",
        "print(f\"Total unique orders: {data['olist_orders_dataset']['order_id'].nunique()}\")\n",
        "\n",
        "# Now, the big one: how many unique *customers*?\n",
        "# I need to use 'customer_unique_id' from the customers table for this.\n",
        "print(f\"Total unique customers: {data['olist_customers_dataset']['customer_unique_id'].nunique()}\")\n",
        "\n",
        "# Let's also check the number of products and sellers.\n",
        "print(f\"Total products: {data['olist_products_dataset']['product_id'].nunique()}\")\n",
        "print(f\"Total sellers: {data['olist_sellers_dataset']['seller_id'].nunique()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQ8hm2rx7Af8",
        "outputId": "79d34e43-90df-4030-8ab3-d0dadf8bd374"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting Step 2: Exploratory Data Analysis ---\n",
            "Total rows in orders table: 99441\n",
            "Total unique orders: 99441\n",
            "Total unique customers: 96096\n",
            "Total products: 32951\n",
            "Total sellers: 3095\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Understanding the Time Range and Order Status ---\")\n",
        "\n",
        "# I need to convert the purchase timestamp to datetime objects to work with them.\n",
        "# I'll make a copy to avoid a SettingWithCopyWarning, which is an annoying pandas thing.\n",
        "orders_df = data['olist_orders_dataset'].copy()\n",
        "\n",
        "# Converting the timestamp column to datetime\n",
        "orders_df['order_purchase_timestamp'] = pd.to_datetime(orders_df['order_purchase_timestamp'])\n",
        "\n",
        "# Let's find the first and last order dates\n",
        "min_date = orders_df['order_purchase_timestamp'].min()\n",
        "max_date = orders_df['order_purchase_timestamp'].max()\n",
        "\n",
        "print(f\"Data ranges from: {min_date} to {max_date}\")\n",
        "print(f\"Total time span: {(max_date - min_date).days} days\")\n",
        "\n",
        "print(\"\\n--- Understanding Order Status ---\")\n",
        "# What's the breakdown of order statuses? This is key for data cleaning.\n",
        "# I probably only want to analyze 'delivered' orders.\n",
        "# Canceled or unavailable orders aren't useful for a churn model.\n",
        "status_counts = orders_df['order_status'].value_counts(normalize=True) * 100\n",
        "\n",
        "print(\"Order Status (Percentage):\")\n",
        "print(status_counts)\n",
        "\n",
        "# That 'delivered' percentage looks high, which is good.\n",
        "# For the rest of the project, I'll make a note to filter for 'delivered' orders."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K55hf_oj-3JE",
        "outputId": "a6611a17-6f7f-477e-d48e-c68e1c82ef21"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Understanding the Time Range and Order Status ---\n",
            "Data ranges from: 2016-09-04 21:15:19 to 2018-10-17 17:30:18\n",
            "Total time span: 772 days\n",
            "\n",
            "--- Understanding Order Status ---\n",
            "Order Status (Percentage):\n",
            "order_status\n",
            "delivered      97.020344\n",
            "shipped         1.113223\n",
            "canceled        0.628513\n",
            "unavailable     0.612423\n",
            "invoiced        0.315765\n",
            "processing      0.302692\n",
            "created         0.005028\n",
            "approved        0.002011\n",
            "Name: proportion, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2.B: Problem Framing - Defining 'Churn'\n",
        "\n",
        "This is the most critical business definition for this project.\n",
        "\n",
        "This isn't a subscription service like Netflix, so churn is tricky. A customer doesn't \"cancel\" a subscription; they just... stop buying.\n",
        "\n",
        "I need to define a time window. If a customer doesn't buy within this window, I'll label them as 'churned'.\n",
        "\n",
        "**My Initial Churn Definition:**\n",
        "A customer is considered **churned** if they do not make another purchase within **90 days** of their last purchase.\n",
        "\n",
        "**Why 90 days?**\n",
        "It's a common business standard (a full quarter). This is a strong, defensible assumption to start with. Later, in Step 3, I can actually analyze the *average time between purchases* to see if 90 days is a good number, but for now, this sets our target.\n",
        "\n",
        "**Snapshot Date:**\n",
        "To calculate this, I'll need a \"snapshot\" or \"reference\" date. I will set this as the day after the last order in the dataset.\n",
        "* Last Order Date: `2018-10-17` (from the code above)\n",
        "* My Snapshot Date: `2018-10-18`"
      ],
      "metadata": {
        "id": "-FTRMkVeBQXy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2.C: Problem Framing - Defining Success\n",
        "\n",
        "If my model works, how do I prove it's valuable to the business? I need to define the Key Performance Indicators (KPIs) we want to improve.\n",
        "\n",
        "**1. Primary Metric: 30-Day Incremental Revenue**\n",
        "This is the *real* goal. It's not just about stopping churn; it's about generating *more revenue* (or stopping revenue loss) than the cost of our intervention. The uplift model in a later step will be key to optimizing this.\n",
        "\n",
        "**2. Secondary Metric (Guardrail): Average Order Value (AOV)**\n",
        "When I send a 20% off coupon, I need to make sure customers don't just start buying smaller, less profitable items. I'll need to track AOV to make sure our intervention isn't accidentally hurting profit.\n",
        "\n",
        "**3. Model Metric: Precision@100**\n",
        "For the model itself, I don't care as much about overall accuracy. I care about the *top 100* customers my model flags. 'Precision@100' will tell me: \"Of the top 100 customers my model *said* would churn, how many *actually* did?\" This is a very practical metric that a marketing team can use."
      ],
      "metadata": {
        "id": "NYzh2nOOBlH6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 3.A: Merging All DataFrames ---\n",
        "print(\"--- Starting Step 3: Data Merging & Feature Engineering ---\")\n",
        "\n",
        "# We'll merge all the tables together.\n",
        "# 1. Start with 'orders' and 'customers'\n",
        "merged_df = pd.merge(\n",
        "    data['olist_orders_dataset'],\n",
        "    data['olist_customers_dataset'],\n",
        "    on='customer_id',\n",
        "    how='left' # Keep all orders, even if customer info is missing (though unlikely)\n",
        ")\n",
        "\n",
        "# 2. Add 'order_items'\n",
        "# One order can have multiple items, so this will make the table longer\n",
        "merged_df = pd.merge(\n",
        "    merged_df,\n",
        "    data['olist_order_items_dataset'],\n",
        "    on='order_id',\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "# 3. Add 'order_payments'\n",
        "merged_df = pd.merge(\n",
        "    merged_df,\n",
        "    data['olist_order_payments_dataset'],\n",
        "    on='order_id',\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "# 4. Add 'order_reviews'\n",
        "merged_df = pd.merge(\n",
        "    merged_df,\n",
        "    data['olist_order_reviews_dataset'],\n",
        "    on='order_id',\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "# 5. Add 'products' to get category names\n",
        "merged_df = pd.merge(\n",
        "    merged_df,\n",
        "    data['olist_products_dataset'],\n",
        "    on='product_id',\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "# 6. Add 'product_category_name_translation' to get English names\n",
        "merged_df = pd.merge(\n",
        "    merged_df,\n",
        "    data['product_category_name_translation'],\n",
        "    on='product_category_name',\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "\n",
        "print(f\"Original merged data shape: {merged_df.shape}\")\n",
        "\n",
        "# Let's inspect the new master table\n",
        "print(\"\\n--- Master Merged DataFrame Head ---\")\n",
        "print(merged_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OBKTmdzQ_oVO",
        "outputId": "1ddb569b-5aea-49b7-8a05-0f0f0842a080"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting Step 3: Data Merging & Feature Engineering ---\n",
            "Original merged data shape: (119143, 37)\n",
            "\n",
            "--- Master Merged DataFrame Head ---\n",
            "                           order_id                       customer_id  \\\n",
            "0  e481f51cbdc54678b7cc49136f2d6af7  9ef432eb6251297304e76186b10a928d   \n",
            "1  e481f51cbdc54678b7cc49136f2d6af7  9ef432eb6251297304e76186b10a928d   \n",
            "2  e481f51cbdc54678b7cc49136f2d6af7  9ef432eb6251297304e76186b10a928d   \n",
            "3  53cdb2fc8bc7dce0b6741e2150273451  b0830fb4747a6c6d20dea0b8c802d7ef   \n",
            "4  47770eb9100c2d0c44946d9cf07ec65d  41ce2a54c0b03bf3443c3d931a367089   \n",
            "\n",
            "  order_status order_purchase_timestamp    order_approved_at  \\\n",
            "0    delivered      2017-10-02 10:56:33  2017-10-02 11:07:15   \n",
            "1    delivered      2017-10-02 10:56:33  2017-10-02 11:07:15   \n",
            "2    delivered      2017-10-02 10:56:33  2017-10-02 11:07:15   \n",
            "3    delivered      2018-07-24 20:41:37  2018-07-26 03:24:27   \n",
            "4    delivered      2018-08-08 08:38:49  2018-08-08 08:55:23   \n",
            "\n",
            "  order_delivered_carrier_date order_delivered_customer_date  \\\n",
            "0          2017-10-04 19:55:00           2017-10-10 21:25:13   \n",
            "1          2017-10-04 19:55:00           2017-10-10 21:25:13   \n",
            "2          2017-10-04 19:55:00           2017-10-10 21:25:13   \n",
            "3          2018-07-26 14:31:00           2018-08-07 15:27:45   \n",
            "4          2018-08-08 13:50:00           2018-08-17 18:06:29   \n",
            "\n",
            "  order_estimated_delivery_date                customer_unique_id  \\\n",
            "0           2017-10-18 00:00:00  7c396fd4830fd04220f754e42b4e5bff   \n",
            "1           2017-10-18 00:00:00  7c396fd4830fd04220f754e42b4e5bff   \n",
            "2           2017-10-18 00:00:00  7c396fd4830fd04220f754e42b4e5bff   \n",
            "3           2018-08-13 00:00:00  af07308b275d755c9edb36a90c618231   \n",
            "4           2018-09-04 00:00:00  3a653a41f6f9fc3d2a113cf8398680e8   \n",
            "\n",
            "   customer_zip_code_prefix  ... review_answer_timestamp  \\\n",
            "0                      3149  ...     2017-10-12 03:43:48   \n",
            "1                      3149  ...     2017-10-12 03:43:48   \n",
            "2                      3149  ...     2017-10-12 03:43:48   \n",
            "3                     47813  ...     2018-08-08 18:37:50   \n",
            "4                     75265  ...     2018-08-22 19:07:58   \n",
            "\n",
            "   product_category_name  product_name_lenght product_description_lenght  \\\n",
            "0  utilidades_domesticas                 40.0                      268.0   \n",
            "1  utilidades_domesticas                 40.0                      268.0   \n",
            "2  utilidades_domesticas                 40.0                      268.0   \n",
            "3             perfumaria                 29.0                      178.0   \n",
            "4             automotivo                 46.0                      232.0   \n",
            "\n",
            "  product_photos_qty product_weight_g  product_length_cm  product_height_cm  \\\n",
            "0                4.0            500.0               19.0                8.0   \n",
            "1                4.0            500.0               19.0                8.0   \n",
            "2                4.0            500.0               19.0                8.0   \n",
            "3                1.0            400.0               19.0               13.0   \n",
            "4                1.0            420.0               24.0               19.0   \n",
            "\n",
            "   product_width_cm product_category_name_english  \n",
            "0              13.0                    housewares  \n",
            "1              13.0                    housewares  \n",
            "2              13.0                    housewares  \n",
            "3              19.0                     perfumery  \n",
            "4              21.0                          auto  \n",
            "\n",
            "[5 rows x 37 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 3.B: Cleaning the Master Table ---\n",
        "\n",
        "# 1. Filter for 'delivered' orders only\n",
        "# As we decided in Step 2, we can't analyze canceled or unavailable orders.\n",
        "master_df = merged_df[merged_df['order_status'] == 'delivered'].copy()\n",
        "print(f\"Shape after filtering for 'delivered' orders: {master_df.shape}\")\n",
        "\n",
        "# 2. Convert timestamp columns to datetime objects\n",
        "# This is crucial for calculating recency and other time-based features.\n",
        "time_cols = [\n",
        "    'order_purchase_timestamp',\n",
        "    'order_approved_at',\n",
        "    'order_delivered_carrier_date',\n",
        "    'order_estimated_delivery_date',\n",
        "    'order_delivered_customer_date'\n",
        "]\n",
        "\n",
        "print(\"\\nConverting timestamp columns...\")\n",
        "for col in time_cols:\n",
        "    master_df[col] = pd.to_datetime(master_df[col], errors='coerce') # 'coerce' will handle any errors\n",
        "\n",
        "# 3. Handle key missing values\n",
        "# For product_category_name_english, let's fill missing values with 'unknown'\n",
        "master_df['product_category_name_english'] = master_df['product_category_name_english'].fillna('unknown')\n",
        "\n",
        "# 4. Drop duplicate rows (if any)\n",
        "# This can happen during merges, so it's good practice.\n",
        "master_df = master_df.drop_duplicates()\n",
        "print(f\"Shape after dropping duplicates: {master_df.shape}\")\n",
        "\n",
        "# Let's check the dtypes and info\n",
        "print(\"\\n--- Cleaned Master DataFrame Info ---\")\n",
        "master_df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9gRUhG1zCIWM",
        "outputId": "299a2a3a-eab9-4055-a30c-ad4e8b050886"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape after filtering for 'delivered' orders: (115723, 37)\n",
            "\n",
            "Converting timestamp columns...\n",
            "Shape after dropping duplicates: (115723, 37)\n",
            "\n",
            "--- Cleaned Master DataFrame Info ---\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 115723 entries, 0 to 119142\n",
            "Data columns (total 37 columns):\n",
            " #   Column                         Non-Null Count   Dtype         \n",
            "---  ------                         --------------   -----         \n",
            " 0   order_id                       115723 non-null  object        \n",
            " 1   customer_id                    115723 non-null  object        \n",
            " 2   order_status                   115723 non-null  object        \n",
            " 3   order_purchase_timestamp       115723 non-null  datetime64[ns]\n",
            " 4   order_approved_at              115708 non-null  datetime64[ns]\n",
            " 5   order_delivered_carrier_date   115721 non-null  datetime64[ns]\n",
            " 6   order_delivered_customer_date  115715 non-null  datetime64[ns]\n",
            " 7   order_estimated_delivery_date  115723 non-null  datetime64[ns]\n",
            " 8   customer_unique_id             115723 non-null  object        \n",
            " 9   customer_zip_code_prefix       115723 non-null  int64         \n",
            " 10  customer_city                  115723 non-null  object        \n",
            " 11  customer_state                 115723 non-null  object        \n",
            " 12  order_item_id                  115723 non-null  float64       \n",
            " 13  product_id                     115723 non-null  object        \n",
            " 14  seller_id                      115723 non-null  object        \n",
            " 15  shipping_limit_date            115723 non-null  object        \n",
            " 16  price                          115723 non-null  float64       \n",
            " 17  freight_value                  115723 non-null  float64       \n",
            " 18  payment_sequential             115720 non-null  float64       \n",
            " 19  payment_type                   115720 non-null  object        \n",
            " 20  payment_installments           115720 non-null  float64       \n",
            " 21  payment_value                  115720 non-null  float64       \n",
            " 22  review_id                      114862 non-null  object        \n",
            " 23  review_score                   114862 non-null  float64       \n",
            " 24  review_comment_title           13584 non-null   object        \n",
            " 25  review_comment_message         48095 non-null   object        \n",
            " 26  review_creation_date           114862 non-null  object        \n",
            " 27  review_answer_timestamp        114862 non-null  object        \n",
            " 28  product_category_name          114085 non-null  object        \n",
            " 29  product_name_lenght            114085 non-null  float64       \n",
            " 30  product_description_lenght     114085 non-null  float64       \n",
            " 31  product_photos_qty             114085 non-null  float64       \n",
            " 32  product_weight_g               115703 non-null  float64       \n",
            " 33  product_length_cm              115703 non-null  float64       \n",
            " 34  product_height_cm              115703 non-null  float64       \n",
            " 35  product_width_cm               115703 non-null  float64       \n",
            " 36  product_category_name_english  115723 non-null  object        \n",
            "dtypes: datetime64[ns](5), float64(14), int64(1), object(17)\n",
            "memory usage: 33.6+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 3.C: Feature Engineering ---\n",
        "\n",
        "print(\"\\n--- Engineering Customer-Level Features ---\")\n",
        "\n",
        "# 1. Define our Snapshot Date\n",
        "# As decided in Step 2, we'll set this to one day after the last purchase in the dataset.\n",
        "snapshot_date = master_df['order_purchase_timestamp'].max() + pd.Timedelta(days=1)\n",
        "print(f\"Snapshot Date for analysis: {snapshot_date}\")\n",
        "\n",
        "\n",
        "# 2. Group by customer\n",
        "# We create a new DataFrame to hold our customer-level features.\n",
        "# We'll group by 'customer_unique_id' and get the most recent purchase date for each customer.\n",
        "customer_features = master_df.groupby('customer_unique_id')['order_purchase_timestamp'].max().reset_index()\n",
        "customer_features.rename(columns={'order_purchase_timestamp': 'last_purchase_date'}, inplace=True)\n",
        "\n",
        "\n",
        "# --- 3. Engineer RFM Features ---\n",
        "\n",
        "# [R] Recency: How many days ago was their last purchase?\n",
        "customer_features['recency'] = (snapshot_date - customer_features['last_purchase_date']).dt.days\n",
        "\n",
        "# [F] Frequency: How many total orders have they placed?\n",
        "# We count the number of unique 'order_id's for each 'customer_unique_id'\n",
        "frequency_df = master_df.groupby('customer_unique_id')['order_id'].nunique().reset_index()\n",
        "frequency_df.rename(columns={'order_id': 'frequency'}, inplace=True)\n",
        "\n",
        "customer_features = pd.merge(customer_features, frequency_df, on='customer_unique_id', how='left')\n",
        "\n",
        "# [M] Monetary: How much total money have they spent?\n",
        "# We use 'payment_value' for this.\n",
        "monetary_df = master_df.groupby('customer_unique_id')['payment_value'].sum().reset_index()\n",
        "monetary_df.rename(columns={'payment_value': 'monetary'}, inplace=True)\n",
        "\n",
        "customer_features = pd.merge(customer_features, monetary_df, on='customer_unique_id', how='left')\n",
        "\n",
        "\n",
        "# --- 4. Engineer Other Behavioral Features ---\n",
        "\n",
        "# Average Review Score\n",
        "review_df = master_df.groupby('customer_unique_id')['review_score'].mean().reset_index()\n",
        "review_df.rename(columns={'review_score': 'avg_review_score'}, inplace=True)\n",
        "customer_features = pd.merge(customer_features, review_df, on='customer_unique_id', how='left')\n",
        "\n",
        "# Average Number of Payment Installments\n",
        "installments_df = master_df.groupby('customer_unique_id')['payment_installments'].mean().reset_index()\n",
        "installments_df.rename(columns={'payment_installments': 'avg_installments'}, inplace=True)\n",
        "customer_features = pd.merge(customer_features, installments_df, on='customer_unique_id', how='left')\n",
        "\n",
        "# Average Freight Value (shipping cost)\n",
        "freight_df = master_df.groupby('customer_unique_id')['freight_value'].mean().reset_index()\n",
        "freight_df.rename(columns={'freight_value': 'avg_freight_value'}, inplace=True)\n",
        "customer_features = pd.merge(customer_features, freight_df, on='customer_unique_id', how='left')\n",
        "\n",
        "# Total number of items purchased\n",
        "items_df = master_df.groupby('customer_unique_id')['order_item_id'].count().reset_index()\n",
        "items_df.rename(columns={'order_item_id': 'total_items'}, inplace=True)\n",
        "customer_features = pd.merge(customer_features, items_df, on='customer_unique_id', how='left')\n",
        "\n",
        "# Most frequent product category\n",
        "def get_most_frequent(x):\n",
        "    # This is a bit complex, but it finds the most common value.\n",
        "    try:\n",
        "        return x.value_counts().index[0]\n",
        "    except:\n",
        "        return 'unknown'\n",
        "\n",
        "category_df = master_df.groupby('customer_unique_id')['product_category_name_english'].apply(get_most_frequent).reset_index()\n",
        "category_df.rename(columns={'product_category_name_english': 'favorite_category'}, inplace=True)\n",
        "customer_features = pd.merge(customer_features, category_df, on='customer_unique_id', how='left')\n",
        "\n",
        "\n",
        "# Let's inspect our new feature table\n",
        "print(\"\\n--- Final Customer Features Table Head ---\")\n",
        "print(customer_features.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5x21juB_CPGO",
        "outputId": "bbbc2ba7-0a36-4a12-abc9-8ef33090050a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Engineering Customer-Level Features ---\n",
            "Snapshot Date for analysis: 2018-08-30 15:00:37\n",
            "\n",
            "--- Final Customer Features Table Head ---\n",
            "                 customer_unique_id  last_purchase_date  recency  frequency  \\\n",
            "0  0000366f3b9a7992bf8c76cfdf3221e2 2018-05-10 10:56:27      112          1   \n",
            "1  0000b849f77a49e4a4ce2b2a4ca5be3f 2018-05-07 11:11:27      115          1   \n",
            "2  0000f46a3911fa3c0805444483337064 2017-03-10 21:05:03      537          1   \n",
            "3  0000f6ccb0745a6a4b88665a16c9f078 2017-10-12 20:29:41      321          1   \n",
            "4  0004aac84e0df4da2b147fca70cf8255 2017-11-14 19:45:42      288          1   \n",
            "\n",
            "   monetary  avg_review_score  avg_installments  avg_freight_value  \\\n",
            "0    141.90               5.0               8.0              12.00   \n",
            "1     27.19               4.0               1.0               8.29   \n",
            "2     86.22               3.0               8.0              17.22   \n",
            "3     43.62               4.0               4.0              17.63   \n",
            "4    196.89               5.0               6.0              16.89   \n",
            "\n",
            "   total_items favorite_category  \n",
            "0            1    bed_bath_table  \n",
            "1            1     health_beauty  \n",
            "2            1        stationery  \n",
            "3            1         telephony  \n",
            "4            1         telephony  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 3.D: Creating the Churn Label ---\n",
        "\n",
        "# 1. Define the Churn Window (90 days)\n",
        "CHURN_DAYS = 90\n",
        "print(f\"Defining churn as no purchase in the last {CHURN_DAYS} days.\")\n",
        "\n",
        "# 2. Create the label\n",
        "# 'recency' is the number of days since the customer's last purchase.\n",
        "# If 'recency' > 90, it means their last purchase was more than 90 days ago.\n",
        "# Therefore, they are 'churned' as of our snapshot date.\n",
        "# We'll use 1 for churned, 0 for not churned.\n",
        "customer_features['is_churned'] = (customer_features['recency'] > CHURN_DAYS).astype(int)\n",
        "\n",
        "# 3. Check the churn rate\n",
        "# This is a key metric. How many of our customers have churned?\n",
        "churn_rate = customer_features['is_churned'].mean() * 100\n",
        "print(f\"\\nOverall Churn Rate in the dataset: {churn_rate:.2f}%\")\n",
        "print(\"This looks like a very imbalanced dataset, which is normal. We'll have to handle this in the modeling step.\")\n",
        "\n",
        "# 4. Save our final table\n",
        "# This 'analytics_base_table.csv' is what we'll use for all our modeling.\n",
        "customer_features.to_csv('analytics_base_table.csv', index=False)\n",
        "print(\"\\n--- Final 'analytics_base_table.csv' created successfully! ---\")\n",
        "\n",
        "# Let's look at the final table with the churn label\n",
        "print(customer_features.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-NjRPh8fCV7H",
        "outputId": "f727a669-76d2-4369-d241-1a39ff625443"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Defining churn as no purchase in the last 90 days.\n",
            "\n",
            "Overall Churn Rate in the dataset: 80.23%\n",
            "This looks like a very imbalanced dataset, which is normal. We'll have to handle this in the modeling step.\n",
            "\n",
            "--- Final 'analytics_base_table.csv' created successfully! ---\n",
            "                 customer_unique_id  last_purchase_date  recency  frequency  \\\n",
            "0  0000366f3b9a7992bf8c76cfdf3221e2 2018-05-10 10:56:27      112          1   \n",
            "1  0000b849f77a49e4a4ce2b2a4ca5be3f 2018-05-07 11:11:27      115          1   \n",
            "2  0000f46a3911fa3c0805444483337064 2017-03-10 21:05:03      537          1   \n",
            "3  0000f6ccb0745a6a4b88665a16c9f078 2017-10-12 20:29:41      321          1   \n",
            "4  0004aac84e0df4da2b147fca70cf8255 2017-11-14 19:45:42      288          1   \n",
            "\n",
            "   monetary  avg_review_score  avg_installments  avg_freight_value  \\\n",
            "0    141.90               5.0               8.0              12.00   \n",
            "1     27.19               4.0               1.0               8.29   \n",
            "2     86.22               3.0               8.0              17.22   \n",
            "3     43.62               4.0               4.0              17.63   \n",
            "4    196.89               5.0               6.0              16.89   \n",
            "\n",
            "   total_items favorite_category  is_churned  \n",
            "0            1    bed_bath_table           1  \n",
            "1            1     health_beauty           1  \n",
            "2            1        stationery           1  \n",
            "3            1         telephony           1  \n",
            "4            1         telephony           1  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HXUDQyVdCk_h"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}